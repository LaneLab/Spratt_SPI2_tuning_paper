{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6245cc2-edaf-4d77-8bed-9fa7d286dc08",
   "metadata": {},
   "source": [
    "#### goal is to extract all features for each mother cell lineage into xarray format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e893ce-6421-4556-b26b-7772515b3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from trackmatexml import TrackmateXML\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0813a-59c4-48fb-a50c-05f06e384b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup directory to save combined xarray data\n",
    "output_dir = '' # change this to be the dir you want to save the combined datasets in\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# import metadata from google doc and on ferets diameter\n",
    "# ferets diameter data location\n",
    "ferets_csv_location = ''\n",
    "# extract data from google doc - download as csv\n",
    "tracked_data_log = pd.read_csv('')\n",
    "# filter based on 'Done' status - not case sensitive\n",
    "filtered_tracked_data_log = tracked_data_log[tracked_data_log[\"MgM XML file correction status\"].str.contains(\"Done\", na=False,case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672839d2-6da9-43ee-ac45-d2ed9f19ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a function to rename spotheaders in xml files - want to replace CH2 with GFP etc\n",
    "\n",
    "def find_and_replace(input_string, replacements):\n",
    "    \"\"\"\n",
    "    Identifies dictionary keys in the input_string and replaces them with corresponding values.\n",
    "\n",
    "    Parameters:\n",
    "    - input_string (str): The string to process.\n",
    "    - replacements (dict): A dictionary where keys are substrings to find and values are replacements.\n",
    "\n",
    "    Returns:\n",
    "    - str: The modified string with replacements applied.\n",
    "    - list: A list of keys that were found and replaced in the input_string.\n",
    "    \"\"\"\n",
    "    # Create a regex pattern to match any of the dictionary keys\n",
    "    pattern = re.compile('|'.join(re.escape(key) for key in replacements.keys()))\n",
    "\n",
    "    # List to store keys that were found and replaced\n",
    "    found_keys = []\n",
    "\n",
    "    # Function to replace matched keys with corresponding values\n",
    "    def replace_match(match):\n",
    "        matched_key = match.group(0)\n",
    "        found_keys.append(matched_key)\n",
    "        return replacements[matched_key]\n",
    "\n",
    "    # Perform the substitution\n",
    "    result_string = pattern.sub(replace_match, input_string)\n",
    "\n",
    "    return result_string, found_keys\n",
    "\n",
    "# create a channel_dict that matches the CH1 etc used in trackmate to actual channel names\n",
    "channel_dict = {'CH1': 'MASKS', 'CH2': 'GFP', 'CH3': 'TRITC', 'CH4': 'PHASE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8636e-15fe-42c4-9114-df2c55ef5929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parent_data_path = '/Volumes/salmonella/users/madison/'\n",
    "frame_interval = 5 # mins\n",
    "expt_dates = ['20240809', '20240918', '20241028']\n",
    "# supply a string to identify the correct xml file to load as there is usually >1 per folder\n",
    "xml_file_label = \"*FINAL_mgm.xml\"\n",
    "# list to store xarray datasets for each position\n",
    "datasets_list = []\n",
    "for expt in expt_dates:\n",
    "    print(f\"Processing data for {expt}:\")\n",
    "    # create the path to the data\n",
    "    data_path = Path(os.path.join(parent_data_path, str(expt)+'_DIMM', 'Channel_Crops'))\n",
    "    subfolders_to_process = filtered_tracked_data_log.loc[filtered_tracked_data_log['Date']==int(expt), 'File Name'].tolist()\n",
    "    subfolder_paths = [d for d in data_path.iterdir() if d.is_dir()]\n",
    "    subfolder_paths = [s for s in subfolder_paths if s.name in subfolders_to_process]\n",
    "\n",
    "    for subfolder in subfolder_paths:\n",
    "        \n",
    "        # extract mc lineage trackname \n",
    "        date_expt = int(expt)  # date in google docs is int64 not string\n",
    "        mc_lineage_tracknumber=filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'Mother_Lineage_Trackname_03_08'].values[0].astype(int)\n",
    "        mc_lineage_trackname= 'Track_'+str(mc_lineage_tracknumber)\n",
    "        channel_width=filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'channel_width'].values[0].astype(float)\n",
    "        # extract xml file \n",
    "        xml_files = list(subfolder.glob(\"*FINAL_mgm.xml\"))\n",
    "        if len(xml_files) == 1:\n",
    "            xml_file_path = subfolder.joinpath(xml_files[0])\n",
    "            \n",
    "            tmxml = TrackmateXML()\n",
    "            tmxml.loadfile(xml_file_path)\n",
    "            \n",
    "            # Step 0: get data for mother cell lineage\n",
    "            tracks = tmxml.analysetrack(mc_lineage_trackname, duplicate_split=False, break_split=False)\n",
    "            # because trackmatexml does this weird thing of adding the parent to the start of the daughter track we need to remove\n",
    "            # the first spotid from every track that is not the parent\n",
    "            for track in tracks:\n",
    "                if track.parent != 0:\n",
    "                    track.spotids = track.spotids[1:]\n",
    "\n",
    "            # Step1: check if a fake cell was added and if yes - remove it\n",
    "            check_fake_cell_added_xml = filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'Division issue?'].str.lower().values\n",
    "            if check_fake_cell_added_xml == 'yes':\n",
    "            \n",
    "                fake_spot_id_to_remove = 100000 # note we give it this same number all the time\n",
    "                for track in tracks:\n",
    "                    # Filter out the spot_id_to_remove from the spotids array\n",
    "                    track.spotids = np.array([sid for sid in track.spotids if sid != fake_spot_id_to_remove], dtype=track.spotids.dtype)\n",
    "\n",
    "            # Step 1a: \n",
    "\n",
    "            # Step 2: extract all data for spotIDs into dictionary - also rename spotheaders to incorporate channel name\n",
    "            spotheader_data_dict = defaultdict(list)\n",
    "\n",
    "            for spot_header in tmxml.spotheader:\n",
    "                for track in tracks:\n",
    "                    updated_spot_header_name, _ = find_and_replace(spot_header, channel_dict)\n",
    "                    spotheader_data_dict[updated_spot_header_name].append(tmxml.getproperty(track.spotids, spot_header))\n",
    "\n",
    "\n",
    "            # Step3: check if a frame had to be added and if yes then extract data for mother cell and add it to the spotheader_data_dict\n",
    "            # cells and parents extracted directly from tracked won't change\n",
    "            \n",
    "            check_extra_frame=filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'Needs frame appended'].str.lower().values[0]\n",
    "            frame_number=filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'Transition Frame 0 in TRACKMATE'].values[0]\n",
    "            \n",
    "            if check_extra_frame =='yes':\n",
    "                \n",
    "                csv_path = subfolder / 'spots.csv'    \n",
    "                extra_spot_data = pd.read_csv(csv_path)\n",
    "                # the first 3 rows probably need to be deleted in all csvs but I will check for ID in label column and only delete the row if it is not found\n",
    "                check_remove_rows = extra_spot_data.iloc[:3]['LABEL'].str.contains('ID', case=False, na=False)\n",
    "                extra_spot_data = pd.concat([extra_spot_data.iloc[:3][check_remove_rows], extra_spot_data.iloc[3:]]).reset_index(drop=True)\n",
    "                # convert format of columns so they are int or floats\n",
    "                cols_as_int32 = ['ID']\n",
    "                cols_as_object = ['LABEL']\n",
    "                for column in extra_spot_data.columns:\n",
    "                    if column in cols_as_int32:\n",
    "                        # do this rather than just using .astype to avoid issues with nans or empty values\n",
    "                        extra_spot_data[column] = pd.to_numeric(extra_spot_data[column], errors='coerce').astype('int32')\n",
    "                    elif column not in cols_as_object:\n",
    "                        extra_spot_data[column] = pd.to_numeric(extra_spot_data[column], errors='coerce').astype('float64')\n",
    "                # find mother cell - should be lowest y when sort - and then extract that row\n",
    "                extra_spot_data=extra_spot_data.sort_values(by='POSITION_Y').reset_index(drop=True)\n",
    "                # create new df to store mc data\n",
    "                # this solves the issue for this position - mother cell died so second cell was tracked\n",
    "                #if (date_expt == 20241028) and (subfolder.name == 'XY19_crop5'):\n",
    "                    ##else:\n",
    "                extra_mc_data = extra_spot_data.iloc[[0]]\n",
    "                # delete MANUAL_SPOT_COLOR and LABEL columns as they are not in full trackmate df\n",
    "                extra_mc_data.drop(columns=['MANUAL_SPOT_COLOR','LABEL', 'TRACK_ID'], inplace=True)\n",
    "                # change frame from 0 to be frame number listed in google doc\n",
    "                extra_mc_data['FRAME']=frame_number \n",
    "                # insert column for ROI_N_POINTS as this wasn't included in single-timepoint data from TrackMate - set to np.nan\n",
    "                extra_mc_data['ROI_N_POINTS']=np.nan\n",
    "                # update column names of dataframe to match renaming of columns done in step 2 for spotheader_data_dict\n",
    "                for col in extra_mc_data.columns:\n",
    "                    updated_name, _ = find_and_replace(col, channel_dict)\n",
    "                    extra_mc_data.rename(columns={col: updated_name}, inplace=True)\n",
    "                # now that the column names match add the data for the extra frame to the first np.array based on matching column name with \n",
    "                # key in spotheader_data_dict\n",
    "                for key in spotheader_data_dict:\n",
    "                    if key in extra_mc_data.columns:\n",
    "                        # Get the value from the DataFrame for the current key\n",
    "                        extra_mc_data_value = extra_mc_data.at[0, key]\n",
    "                                \n",
    "                        # Insert this value at the beginning of the first NumPy array in the list\n",
    "                        spotheader_data_dict[key][0] = np.insert(spotheader_data_dict[key][0], 0, extra_mc_data_value)\n",
    "\n",
    "            # Step 4: add ferets diameter data extracted using regionprops\n",
    "            \n",
    "            ferets_data_df = pd.read_csv(ferets_csv_location)\n",
    "            # first subset the regionprops data to only include relevant info for expt and position\n",
    "            ferets_data_df_subset = ferets_data_df[(ferets_data_df[\"Expt\"] == date_expt) & (ferets_data_df[\"Position_crop\"] == subfolder.name)]\n",
    "            # sometimes there are issues in mismatches between centroid values due to differences at 10th decimal place for e.g. to avoid this we will\n",
    "            # round the values before comparing. I chose 4 decimal places somewhat arbitrarily \n",
    "            decimal_places = 4 # how to do the rounding\n",
    "            # round the df centroid data \n",
    "            ferets_data_df_subset[\"POSITION_X\"] = ferets_data_df_subset[\"POSITION_X\"].round(decimal_places)\n",
    "            ferets_data_df_subset[\"POSITION_Y\"] = ferets_data_df_subset[\"POSITION_Y\"].round(decimal_places)\n",
    "            # create a lookup dictionary from df\n",
    "            lookup = {(row.POSITION_X, row.POSITION_Y, row.frame): row.feret_diameter_max for row in ferets_data_df_subset.itertuples(index=False)}\n",
    "            # use lookup to create new ferets key in spotheader_data_dict with values for each track across frames\n",
    "            # also round centroid values  extracted from dict - but not in place \n",
    "            spotheader_data_dict[\"feret_diameter\"] = [\n",
    "                np.array([lookup.get((round(x, decimal_places), round(y, decimal_places), t), np.nan) for x, y, t in zip(spotheader_data_dict[\"POSITION_X\"][i], spotheader_data_dict[\"POSITION_Y\"][i], \n",
    "                                                                           spotheader_data_dict[\"FRAME\"][i])])\n",
    "                for i in range(len(spotheader_data_dict[\"FRAME\"]))\n",
    "            ]\n",
    "            # there will be some nans as the ferets from regionprops was only calculated on relevant frames but trackmate data is on all frames\n",
    "            # later we reduce the trackmate data to only include relevant frames so this will get rid of these nan\n",
    "\n",
    "            # step 5 reset frames based on end and start\n",
    "            # first extract start and end frames from google doc\n",
    "            start_frame=filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'Transition Frame 0 in TRACKMATE'].values[0].astype(int)\n",
    "            end_frame = filtered_tracked_data_log.loc[(filtered_tracked_data_log['Date'] == date_expt) & (filtered_tracked_data_log['File Name'] == subfolder.name), 'TrackmateEndFrame'].values[0].astype(int)\n",
    "            num_frames = end_frame-start_frame+1\n",
    "            \n",
    "            # convert frames to int and then reset to 0 using start_frame\n",
    "            spotheader_data_dict['FRAME'] = [arr.astype(int) - start_frame for arr in spotheader_data_dict['FRAME']]\n",
    "\n",
    "            # step 6: insert a time column based on frame interval - this doesn't really need to be done here but it might be useful\n",
    "\n",
    "            spotheader_data_dict['Time (mins)'] = [frame_array * frame_interval for frame_array in spotheader_data_dict['FRAME']]\n",
    "\n",
    "            # Step 7: extract variables that don't change over time - cells, parents. Note these are not individual cells at each timepoint but \n",
    "            # the cell ID of the subtrack - this is why it is not affected by the adding back of a frame\n",
    "            cells = [track.cell for track in tracks]\n",
    "            parents = [track.parent for track in tracks]\n",
    "\n",
    "            # Step 8: find cells that start after max frames and then remove all of these arrays from spotheader_dict and then also from cells, parents\n",
    "\n",
    "            arrays_to_remove = []\n",
    "            for i, frame_subset in enumerate(spotheader_data_dict['FRAME']):\n",
    "            \n",
    "                # for each array check if the first value is within the range \n",
    "                if frame_subset[0] > num_frames-1:\n",
    "                    arrays_to_remove.append(i)\n",
    "             \n",
    "            for key in spotheader_data_dict:\n",
    "                \n",
    "                #spotheader_data_dict[key] = [arr for i, arr in enumerate(spotheader_data_dict[key]) if i not in to_delete]\n",
    "            \n",
    "                for i in sorted(arrays_to_remove, reverse=True):\n",
    "                    del spotheader_data_dict[key][i]\n",
    "                            \n",
    "            # now fix cells and parents to remove those that are no longer in spotheader_data_dict\n",
    "            for i in sorted(arrays_to_remove, reverse=True):\n",
    "                del cells[i]\n",
    "                del parents[i]   \n",
    "\n",
    "    \n",
    "            # Step 9: for a lot of data we need to store it in an array that will always be same size but for each cell it will only fill certain frames in the array\n",
    "            # it might be easier to make an array, fill it with 0 where cell should be and nan where it was not present and then use it as a template for each \n",
    "            # feature that is measured\n",
    "            \n",
    "            holder_array = np.full((len(cells), num_frames), np.nan, dtype=np.float32)  \n",
    "            \n",
    "            for i, frame_subset in enumerate(spotheader_data_dict['FRAME']):\n",
    "            \n",
    "                # check if there are frames values to remove\n",
    "                exceed_indices = np.where(frame_subset > num_frames-1)[0] # this is because it is 0 indexed\n",
    "            \n",
    "                if exceed_indices.size > 0:\n",
    "                    # Get the first index where the element exceeds the threshold\n",
    "                    first_exceed_index = exceed_indices[0]\n",
    "                \n",
    "                    # Slice the frame array up to this index (excluding the exceeding element)\n",
    "                    new_frame_arr = frame_subset[:first_exceed_index]\n",
    "               \n",
    "                else:\n",
    "                    # If no elements exceed the threshold, keep the array unchanged\n",
    "            \n",
    "                    new_frame_arr = frame_subset.copy()\n",
    "              \n",
    "            \n",
    "                holder_array[i, new_frame_arr] = 0\n",
    "\n",
    "\n",
    "            # Step 10: create arrays of data want to store in xarray format\n",
    "            \n",
    "            # gfp_median_intensity\n",
    "            gfp_median_intensity_data = holder_array.copy()\n",
    "            for i in range(len(spotheader_data_dict[\"MEDIAN_INTENSITY_GFP\"])):\n",
    "                t = spotheader_data_dict[\"FRAME\"][i]\n",
    "                v = spotheader_data_dict[\"MEDIAN_INTENSITY_GFP\"][i]\n",
    "                mask = t < num_frames\n",
    "                t_filtered = t[mask]\n",
    "                v_filtered = v[mask]\n",
    "                gfp_median_intensity_data[i, t_filtered] = v_filtered\n",
    "            \n",
    "            # tritc_median_intensity\n",
    "            tritc_median_intensity_data = holder_array.copy()\n",
    "            for i in range(len(spotheader_data_dict[\"MEDIAN_INTENSITY_TRITC\"])):\n",
    "                t = spotheader_data_dict[\"FRAME\"][i]\n",
    "                v = spotheader_data_dict[\"MEDIAN_INTENSITY_TRITC\"][i]\n",
    "                mask = t < num_frames\n",
    "                t_filtered = t[mask]\n",
    "                v_filtered = v[mask]\n",
    "                tritc_median_intensity_data[i, t_filtered] = v_filtered\n",
    "            \n",
    "            # area\n",
    "            area_data = holder_array.copy()\n",
    "            for i in range(len(spotheader_data_dict[\"AREA\"])):\n",
    "                t = spotheader_data_dict[\"FRAME\"][i]\n",
    "                v = spotheader_data_dict[\"AREA\"][i]\n",
    "                mask = t < num_frames\n",
    "                t_filtered = t[mask]\n",
    "                v_filtered = v[mask]\n",
    "                area_data[i, t_filtered] = v_filtered\n",
    "            \n",
    "            # feret_diameter\n",
    "            feret_diameter_data = holder_array.copy()\n",
    "            for i in range(len(spotheader_data_dict[\"feret_diameter\"])):\n",
    "                t = spotheader_data_dict[\"FRAME\"][i]\n",
    "                v = spotheader_data_dict[\"feret_diameter\"][i]\n",
    "                mask = t < num_frames\n",
    "                t_filtered = t[mask]\n",
    "                v_filtered = v[mask]\n",
    "                feret_diameter_data[i, t_filtered] = v_filtered\n",
    "            \n",
    "            # centroid_x \n",
    "            centroid_x_data = holder_array.copy()\n",
    "            for i in range(len(spotheader_data_dict[\"POSITION_X\"])):\n",
    "                t = spotheader_data_dict[\"FRAME\"][i]\n",
    "                v = spotheader_data_dict[\"POSITION_X\"][i]\n",
    "                mask = t < num_frames\n",
    "                t_filtered = t[mask]\n",
    "                v_filtered = v[mask]\n",
    "                centroid_x_data[i, t_filtered] = v_filtered\n",
    "               \n",
    "            # centroid_y 'POSITION_Y'\n",
    "            centroid_y_data = holder_array.copy()\n",
    "            for i in range(len(spotheader_data_dict[\"POSITION_Y\"])):\n",
    "                t = spotheader_data_dict[\"FRAME\"][i]\n",
    "                v = spotheader_data_dict[\"POSITION_Y\"][i]\n",
    "                mask = t < num_frames\n",
    "                t_filtered = t[mask]\n",
    "                v_filtered = v[mask]\n",
    "                centroid_y_data[i, t_filtered] = v_filtered\n",
    "\n",
    "            # Step 11: add daughter information\n",
    "            # find time when each cell appears - note that any array still in the data_dict has to have appeared in our filtered frames\n",
    "            # as any array starting after the endpoint has already been removed. We don't need to do any masking here since we only care\n",
    "            # about the frame the cell appears at\n",
    "            new_cell_appearances = [int(arr[0]) for arr in spotheader_data_dict['FRAME']] \n",
    "            daughters = holder_array.copy()\n",
    "            for cell, parent, appearance_time in zip(cells, parents, new_cell_appearances):\n",
    "                if parent != 0:\n",
    "                    # here we need to use parent-1 as if parent is cell 1 it should be in row 0 of the array \n",
    "                    # appearance time is already based on zero indexing (i.e. frames go from 0-216) so we don't need to adjust that\n",
    "                    daughters[parent-1, appearance_time]=cell \n",
    "\n",
    "\n",
    "            # Step 12: combine all dataarrays into a dataset for that lineage\n",
    "            # metadata to store as coords\n",
    "            position = subfolder.name\n",
    "            trackname = mc_lineage_trackname\n",
    "            channel_width= channel_width \n",
    "            num_cells=len(cells)\n",
    "            timepoints = np.arange(0, num_frames*frame_interval, frame_interval)\n",
    "\n",
    "            # convert all lineage data into xarray dataarrays\n",
    "            gfp_median_intensity_array = xr.DataArray(gfp_median_intensity_data, dims=(\"cell\", \"time\"), name=\"GFP_median_intensity\")\n",
    "            tritc_median_intensity_array = xr.DataArray(tritc_median_intensity_data, dims=(\"cell\", \"time\"), name=\"TRITC_median_intensity\")\n",
    "            area_array = xr.DataArray(area_data, dims=(\"cell\", \"time\"), name=\"Area\")\n",
    "            feret_diameter_array = xr.DataArray(feret_diameter_data, dims=(\"cell\", \"time\"), name=\"feret_diameter\")\n",
    "            centroid_x_array = xr.DataArray(centroid_x_data, dims=(\"cell\", \"time\"), name=\"centroid_x\")\n",
    "            centroid_y_array = xr.DataArray(centroid_y_data, dims=(\"cell\", \"time\"), name=\"centroid_y\")\n",
    "            parents_array = xr.DataArray(parents, dims=\"cell\", name=\"parents\")\n",
    "            daughters_array = xr.DataArray(daughters, dims=(\"cell\", \"time\"), name=\"daughters\")\n",
    "            \n",
    "            # now create a dataset of those dataarrays\n",
    "            lineage_ds = xr.Dataset(\n",
    "                data_vars={\n",
    "                    \"GFP_median_intensity\": gfp_median_intensity_array,\n",
    "                    \"TRITC_median_intensity\": tritc_median_intensity_array,\n",
    "                    \"Area\": area_array,\n",
    "                    \"feret_diameter\": feret_diameter_array,\n",
    "                    \"centroid_x\": centroid_x_array,\n",
    "                    \"centroid_y\": centroid_y_array,\n",
    "                    \"parents\": parents_array,\n",
    "                    \"daughters\": daughters_array,\n",
    "                },\n",
    "            \n",
    "                coords={\"experiment\": expt, \"position\": position, \"track\": trackname, \n",
    "                        \"cell\": cells, \"time\": timepoints, \"channel_width\": channel_width},\n",
    "            )\n",
    "            # save the dataset\n",
    "            lineage_ds.to_netcdf(Path(subfolder / 'lineage_dataset.nc'))\n",
    "            # append the dataset for each position to the datasets_list\n",
    "            datasets_list.append(lineage_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e2193-667b-4e46-9650-da3ab475ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used to store all the ds in one dictionary and create a dataframe of metadata for all ds\n",
    "def datasets_to_dict(datasets_list):\n",
    "    # create a dict to store all the datasets and a list for the metadata (experiment, position, track)\n",
    "    all_dataset_dict = {}\n",
    "    all_metadata = []\n",
    "\n",
    "    for ds in datasets_list:\n",
    "        experiment = ds.coords[\"experiment\"].item()\n",
    "        position = ds.coords[\"position\"].item()\n",
    "        track = ds.coords[\"track\"].item()\n",
    "        channel_width = ds.coords[\"channel_width\"].item()\n",
    "\n",
    "        # the key for each ds in the dict is the unique combination of experiment, position, track\n",
    "        # note they are split by two underscores \n",
    "        key = f\"{experiment}__{position}__{track}\"\n",
    "        all_dataset_dict[key] = ds\n",
    "\n",
    "        all_metadata.append({\n",
    "            \"unique_ID\": key,\n",
    "            \"experiment\": experiment,\n",
    "            \"position\": position,\n",
    "            \"track\": track,\n",
    "            \"channel_width\":channel_width\n",
    "        })\n",
    "\n",
    "    all_metadata_df = pd.DataFrame(all_metadata)\n",
    "    return all_dataset_dict, all_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3120f-0478-4f9d-ba9f-ac98472962ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data\n",
    "# combine individual ds into a dictionary and create a df with metadata on experiment, position, track\n",
    "ds_all_dict, all_metadata_df = datasets_to_dict(datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06ed45-5893-4f6c-b62b-4188a467ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary and metadata dataframe\n",
    "# use pickle to save the dictionary\n",
    "with open(os.path.join(output_dir, 'all_datasets_dict_0530.pkl'), 'wb') as f:\n",
    "    pickle.dump(ds_all_dict, f)\n",
    "    \n",
    "all_metadata_df.to_csv(os.path.join(output_dir, 'all_metadata_0530.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04198b-dc3a-4cb1-8ea1-9ed64cc4f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list[0].daughters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fa76f-7c86-4a06-b4f1-3397a7d8ef41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
