{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58cfc3-34ad-4945-96b9-ac3a816cc2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from trackmatexml import TrackmateXML\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# makes figures look better in Jupyter\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"ticks\")\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098e994-ae48-425f-80ef-0455bfb89f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "output_dir = '/Volumes/salmonella/users/madison/2024_DIMM_MultirepAnalysis/combined_datasets'\n",
    "plot_output = '/Volumes/salmonella/users/madison/2024_DIMM_MultirepAnalysis/LineagePlotting/Final_Plots'\n",
    "with open(os.path.join(output_dir, 'all_datasets_dict.pkl'), 'rb') as f:\n",
    "    ds_all_dict = pickle.load(f)\n",
    "\n",
    "all_metadata_df = pd.read_csv(os.path.join(output_dir, 'all_metadata.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5ccf5-5e59-46bc-b36d-45e182cc6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check everything looks ok\n",
    "all_metadata_df.head()\n",
    "print(len(all_meta_data_df))\n",
    "print(len(ds_all_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb721ca9-5d40-4076-a3f8-4d77a68af00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional subsetting if you need to\n",
    "keys_to_include = { '20240809__XY05_crop3__Track_157' }\n",
    "subset_dict = {k: ds_all_dict[k] for k in keys_to_include}\n",
    "subset_dict['20240809__XY05_crop3__Track_157']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435318c-1e6f-4004-9231-b868a50e46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##HARDCODED INFO BASED ON PRIOR ANALYSES IN BACKGROUND AND T=0 INTENSITIES\n",
    "#Do not change, use these #'s in any downstream analysis\n",
    "gfp_background_dict = {'20240809':0, '20240918':99.11, '20241028':66.29}\n",
    "ruby_background_dict = {'20240809':6.7898, '20240918':155.8976, '20241028':0}\n",
    "gfp_threshold = 1515.608\n",
    "pixel_conversion = 0.13\n",
    "log_gfp_th = np.log(gfp_threshold)\n",
    "IDs_to_drop = ['20241028__XY24_crop2__Track_5', '20240918__XY07_crop1__Track_2'] \n",
    "#remove any keys that shouldn't be included in dataset,  one here was a perfect duplicate lineage (likely made two of the same crop), other had 3 top cells die and no mother div. \n",
    "for id in IDs_to_drop:\n",
    "        if id in ds_all_dict:\n",
    "            del ds_all_dict[id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bf4c3-1a8b-4fdf-b777-219c67aee684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_processed_data_to_dataset_dict(dataset_dict, metadata_df, data_var_to_modify, new_var_name, \n",
    "                                    interpolate = True, smooth = True, log_trans = False, \n",
    "                                    fluor_background_dict = None, window_size = 3, save_dict=False):\n",
    "    \"\"\"\n",
    "    goal here is to add a new data variable to the combined dataset dictionary\n",
    "    dataset_dict is the dictionary of all the xarray datasets for each position\n",
    "    data_var_to_modify is the data you want to you want to modify to create the new data variable\n",
    "    new_var_name is name of the new data variable\n",
    "    window_size is the size of moving window to use for the smoothing\n",
    "    save_dict set to True if you want to save the updated dictionary - default is False - it is probably best to test things out first\n",
    "    and you can always resave the dictionary outside of the function\n",
    "    \"\"\"\n",
    "\n",
    "    for unique_id, ds in dataset_dict.items():\n",
    "\n",
    "        print(f\"Running analysis on lineage: {unique_id}\")\n",
    "        # ds is now our dataset - we don't need to select it\n",
    " \n",
    "        # first get the GFP data or whatever variable you want to modify\n",
    "        data = ds[data_var_to_modify]\n",
    "        # normalize the data based on the background\n",
    "        experiment_name = ds.coords['experiment'].item()\n",
    "        print(f\"Experiment name for {unique_id}: '{experiment_name}'\")\n",
    "        if fluor_background_dict != None:\n",
    "            background_value_to_add = fluor_background_dict.get(experiment_name, 0)\n",
    "            print(f\"{unique_id} background = {background_value_to_add}\")\n",
    "            background_adjusted_data = data + background_value_to_add\n",
    "        else:\n",
    "            background_adjusted_data = data\n",
    "        # interpolate the data - set fill_value to None to not go beyond edges I think\n",
    "        if interpolate == True:\n",
    "            data_interpolated = background_adjusted_data.interpolate_na(dim='time', method='linear', fill_value=None,\n",
    "                                                                    max_gap=1, use_coordinate=False)\n",
    "        else:\n",
    "            data_interpolated = background_adjusted_data\n",
    "        if smooth == True: \n",
    "            data_smoothed = data_interpolated.rolling(time=window_size, center=False, min_periods=1).mean()\n",
    "        else:\n",
    "            data_smoothed = data_interpolated\n",
    "        \n",
    "        # add the data to the ds\n",
    "        ds[new_var_name] = data_smoothed\n",
    "        #log transform and make a new array if you want one\n",
    "        if log_trans ==True:\n",
    "            ds[new_var_name+'_transformed'] = xr.apply_ufunc(np.log1p, ds[new_var_name])\n",
    "        else:\n",
    "            continue\n",
    "    # save the data if save_dict = True\n",
    "    if save_dict:\n",
    "        with open(os.path.join(output_dir, 'all_datasets_dict.pkl'), 'wb') as f:\n",
    "            pickle.dump(ds_all_dict, f)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa078b1-6d02-487a-afa0-8ce8e0b5fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixeltoum_conversion(dataset_dict, metadata_df, data_var_to_modify, new_var_name, pixel_conversion, save_dict=False):\n",
    "    \"\"\"\n",
    "    convert a size based metric to um based on pixel conversion\n",
    "    \"\"\"\n",
    "\n",
    "    for unique_id, ds in dataset_dict.items():\n",
    "\n",
    "        print(f\"Running analysis on lineage: {unique_id}\")\n",
    "        data = ds[data_var_to_modify]\n",
    "        converted_data = data*pixel_conversion\n",
    "        ds[new_var_name] = converted_data\n",
    " \n",
    "    # save the data if save_dict = True\n",
    "    if save_dict:\n",
    "        with open(os.path.join(output_dir, 'all_datasets_dict.pkl'), 'wb') as f:\n",
    "            pickle.dump(ds_all_dict, f)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca2a68-1e5d-4fe7-9045-d2326361f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixeltoum_conversion(ds_all_dict, all_metadata_df, 'feret_diameter', 'feret_diameter_um', pixel_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445554b7-6c17-4120-b19b-32b53facbcd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_processed_data_to_dataset_dict(ds_all_dict, all_metadata_df, 'GFP_median_intensity', \n",
    "                                    'GFP_median_intensity_processed', fluor_background_dict = gfp_background_dict, log_trans = True,  \n",
    "                                    window_size = 3, save_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e66db8-e27f-4d47-88eb-8bce104cb1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_processed_data_to_dataset_dict(ds_all_dict, all_metadata_df, 'TRITC_median_intensity', \n",
    "                                    'TRITC_median_intensity_processed', fluor_background_dict = ruby_background_dict, log_trans = True,  \n",
    "                                    window_size = 3, save_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfd2cb-b437-4312-a58e-af31ad15174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mother_variables(dataset_dict, metadata, timepoints, variables=None):\n",
    "    \"\"\" \n",
    "    dataset_dict is the dictionary of all the xarray datasets for each position\n",
    "    metadata is the df that stores all the metadata for each positions\n",
    "    timepoints is the list of timepoints you are measuring over - since it is the same for every lineage it makes more sense to supply it\n",
    "    than to extract it each time\n",
    "    variables should be a list ['GFP_median_intensity', 'Area']. List can have one or more elements\n",
    "    \"\"\"\n",
    "    \n",
    "    all_mothers = []\n",
    "    for unique_id, ds in dataset_dict.items():\n",
    "    \n",
    "        print(f\"Running analysis on lineage: {unique_id}\")\n",
    "        # ds is now our dataset - we don't need to select it\n",
    "        # first find the corresponding metadata - we could split the key name but I feel like this could lead to unanticipated problems\n",
    "        ds_metadata = metadata.loc[metadata[\"unique_ID\"] == unique_id].iloc[0]\n",
    "\n",
    "        # find mother cell - based on trackmate it should always be the cell that has parent = 0\n",
    "        mother_cell = ds.parents.where(ds.parents == 0, drop=True).cell.values\n",
    "        # this shouldn't happen but print an error if there is no mother cell\n",
    "        if len(mother_cell) == 0:\n",
    "            print(f\"No mother cell found in {ds_metadata.experiment,}, {ds_metadata.position}.\")\n",
    "            continue\n",
    "        mother_cell_idx = mother_cell[0]\n",
    "\n",
    "        # use a dictionary to store data before creating a df \n",
    "        mc_data_dict = {\n",
    "            \"unique_ID\": ds_metadata.unique_ID,\n",
    "            \"experiment\": ds_metadata.experiment,\n",
    "            \"position\": ds_metadata.position,\n",
    "            \"mother_cell\": mother_cell_idx,\n",
    "            \"time\": timepoints\n",
    "        }\n",
    "        \n",
    "        # extract features over time listed in variables\n",
    "        for var in variables:\n",
    "            data_to_store = ds[var].sel(cell=mother_cell_idx).values\n",
    "            # change to 1D shape\n",
    "            data_to_store_reshaped = np.squeeze(data_to_store)\n",
    "            # add to dictionary\n",
    "            mc_data_dict[var] = data_to_store_reshaped\n",
    "\n",
    "        # create a temp df to store the data\n",
    "        df_temp = pd.DataFrame(mc_data_dict)\n",
    "        # store each df_temp in a list\n",
    "        all_mothers.append(df_temp)\n",
    "\n",
    "    df_all = pd.concat(all_mothers, ignore_index=True)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f684db-c7a0-4241-b67e-1b6d65aef9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to select out data on mother cell\n",
    "# since timepoints should be the same for all ds we are just going to extract them from the first entry in the ds_all_dict\n",
    "first_key = next(iter(ds_all_dict))\n",
    "first_dataset = ds_all_dict[first_key]\n",
    "timepoints = first_dataset.coords['time'].values\n",
    "variables_to_extract = [\"GFP_median_intensity_processed_transformed\", 'TRITC_median_intensity_processed_transformed', 'feret_diameter_um']\n",
    "mothers_df = extract_mother_variables(ds_all_dict, all_metadata_df, timepoints, variables=variables_to_extract)\n",
    "mothers_df.to_csv(plot_output + '/mothers_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0472f-cf1a-4bbb-86d6-402f6913d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_calculations(dataset_dict, metadata, data_var_threshold, slope_threshold=5, duration_thresh=5, data_var=None, lifetime_min=72):\n",
    "    \"\"\" \n",
    "    ##Final slope calculations for reporter\n",
    "    this will take a given dataset variable and ID start and stop times. Keep in mind whether you use log transformed data or not. \n",
    "    -the data_var_threshold is where you are considering something 'positive' (ie the GFP+ thresh)\n",
    "    -slope_threshold is the minimum slope it must cross to be considered an increase\n",
    "    -duration_thresh is how many frames it mus drop below that slope_threshold to be considered the end of an increase\n",
    "    -lifetime_min is just a value for categorization at the end - ie you don't want to call something non responsive if it \n",
    "     didn't exist for this amount of time. \n",
    "    \"\"\"\n",
    "    all_slope_data = []\n",
    "    from scipy.stats import rankdata\n",
    "    for unique_id, ds in dataset_dict.items():\n",
    "        print(f\"Running analysis on lineage: {unique_id}\")\n",
    "        ds_metadata = metadata.loc[metadata[\"unique_ID\"] == unique_id].iloc[0]\n",
    "\n",
    "        slope = ds[data_var].diff(dim='time')\n",
    "        slope_clean = slope.fillna(-np.inf)\n",
    "        above = slope_clean > slope_threshold\n",
    "\n",
    "        cell_records = []\n",
    "\n",
    "        for cell in ds.cell.values:\n",
    "            cell_data = ds[data_var].sel(cell=cell)\n",
    "            parent = ds['parents'].sel(cell=cell).values.item()\n",
    "            lifetime = np.count_nonzero(~np.isnan(cell_data))\n",
    "            positive = cell_data > data_var_threshold\n",
    "            crosses_gfp_th = positive.any().item()\n",
    "            gfp_idx = positive.idxmax(dim='time').values if crosses_gfp_th else np.nan\n",
    "            max_var = cell_data.max(dim='time').values\n",
    "            max_var_time = cell_data.idxmax(dim='time').values\n",
    "            post_max_data = cell_data.sel(time=slice(max_var_time, None))\n",
    "            min_post_max = post_max_data.min(dim='time').values\n",
    "            min_time_after_max = post_max_data.idxmin(dim='time').values\n",
    "            \n",
    "            first_time = cell_data.notnull().idxmax(dim=\"time\").values\n",
    "            print(\"first_time:\", first_time)\n",
    "            print(\"first_time in ds['time']:\", first_time in ds['time'].values)\n",
    "            print(\"Check if value exists:\", ds['centroid_y'].sel(time=first_time, cell=cell))\n",
    "\n",
    "            appearance_position_value = ds['centroid_y'].sel(time=first_time, cell=cell).values\n",
    "            print(appearance_position_value)\n",
    "            above_cell = above.sel(cell=cell)\n",
    "            crosses_slope_th = above_cell.any().item()\n",
    "            start_idx = above_cell.idxmax(dim='time').values if crosses_slope_th else np.nan\n",
    "            if not np.isnan(start_idx):\n",
    "                start_idx_var_value = cell_data.sel(time=start_idx).values\n",
    "                start_idx_position_value = ds['centroid_y'].sel(time=start_idx, cell=cell).values\n",
    "                #position_data_all_cells = ds['centroid_y'].sel(time=start_idx) #gives centroid_y array of all cells in the unique_id\n",
    "                #pos_values = position_data_all_cells.values\n",
    "                #ranks = rankdata(pos_values, method='min')  # lower values = lower rank\n",
    "                #cell_names = position_data_all_cells['cell'].values\n",
    "                #rank_dict = dict(zip(cell_names, ranks))\n",
    "                #cell_pos_rank = rank_dict[cell]\n",
    "\n",
    "            else: \n",
    "                start_idx_var_value=np.nan\n",
    "                start_idx_position_value = np.nan\n",
    "\n",
    "\n",
    "            if not np.isnan(gfp_idx) and gfp_idx != first_time:\n",
    "                below = slope.sel(cell=cell) < slope_threshold\n",
    "                rolling_sum = below.rolling(time=duration_thresh).sum()\n",
    "                sustained_below = rolling_sum == duration_thresh\n",
    "                time_1d = ds['time']\n",
    "\n",
    "                valid_after_start = time_1d >= start_idx\n",
    "                sustained_below_after_start = sustained_below.where(valid_after_start, drop=False)\n",
    "                end_idx = sustained_below_after_start.idxmax(dim='time').values if crosses_slope_th else np.nan\n",
    "                if not np.isnan(end_idx):\n",
    "                    end_idx_var_value = cell_data.sel(time=end_idx).values\n",
    "                else: \n",
    "                    end_idx_var_value=np.nan\n",
    "\n",
    "                max_slope = slope_clean.sel(cell=cell).max(dim='time').values\n",
    "                max_slope_idx = slope_clean.sel(cell=cell).argmax(dim='time').values\n",
    "                time_of_max_slope = ds['time'].isel(time=max_slope_idx).values\n",
    "                born_on = False\n",
    "            \n",
    "            elif not np.isnan(gfp_idx) and gfp_idx==first_time:\n",
    "                start_idx = np.nan\n",
    "                start_idx_var_value = np.nan\n",
    "                start_idx_position_value = np.nan\n",
    "                end_idx = np.nan\n",
    "                end_idx_var_value = np.nan\n",
    "                gfp_idx = np.nan\n",
    "                max_slope = np.nan\n",
    "                time_of_max_slope = np.nan\n",
    "                inc_magnitude = np.nan\n",
    "                born_on = True\n",
    "\n",
    "            else: \n",
    "                start_idx = np.nan\n",
    "                start_idx_var_value = np.nan\n",
    "                start_idx_position_value = np.nan\n",
    "                end_idx = np.nan\n",
    "                end_idx_var_value = np.nan\n",
    "                gfp_idx = np.nan\n",
    "                max_slope = np.nan\n",
    "                time_of_max_slope = np.nan\n",
    "                inc_magnitude = np.nan\n",
    "                born_on = False\n",
    "\n",
    "            cell_records.append({\n",
    "                \"unique_ID\": ds_metadata.unique_ID,\n",
    "                \"experiment\": ds_metadata.experiment,\n",
    "                \"position\": ds_metadata.position,\n",
    "                \"track\": ds_metadata.track,\n",
    "                \"cell_id\": cell,\n",
    "                \"parent\": parent,\n",
    "                \"max_slope\": max_slope,\n",
    "                \"appearance_time\" : first_time,\n",
    "                \"time_of_max_slope\": time_of_max_slope,\n",
    "                \"start_inc\": start_idx,\n",
    "                \"start_idx_var_value\": start_idx_var_value, \n",
    "                \"y_pos_start_inc\": start_idx_position_value,\n",
    "                \"y_pos_appearance\": appearance_position_value,\n",
    "                \"gfp_on\": gfp_idx,\n",
    "                \"end_inc\": end_idx,\n",
    "                \"end_idx_var_value\": end_idx_var_value, \n",
    "                \"born_on\": born_on,\n",
    "                \"lifetime\": lifetime, \n",
    "                \"slope_th\": slope_threshold, \n",
    "                \"max_gfp\": max_var, \n",
    "                \"max_gfp_time\": max_var_time,\n",
    "                \"min_post_max\": min_post_max,\n",
    "                \"min_time_post_max\": min_time_after_max\n",
    "            })\n",
    "\n",
    "        all_slope_data.append(pd.DataFrame(cell_records))\n",
    "\n",
    "    df_slope_all = pd.concat(all_slope_data, ignore_index=True)\n",
    "    df_slope_all['switch_like'] = (df_slope_all['start_inc'] < df_slope_all['gfp_on']) & (df_slope_all['gfp_on'] < df_slope_all['end_inc']) \n",
    "    df_slope_all['becomes_positive'] = ((df_slope_all['born_on'] == False) & (df_slope_all['gfp_on'].notna()))\n",
    "    df_slope_all[\"duration\"] = df_slope_all[\"end_inc\"] - df_slope_all[\"start_inc\"]\n",
    "    df_slope_all['magnitude_inc'] = df_slope_all['end_idx_var_value'] - df_slope_all['start_idx_var_value']\n",
    "    \n",
    "    df_slope_all.loc[df_slope_all['born_on'] == True, 'category'] = None\n",
    "    df_slope_all.loc[((df_slope_all['switch_like'] == True) & (df_slope_all['lifetime'] > lifetime_min)), 'category'] = 'switch'\n",
    "    df_slope_all.loc[((df_slope_all['switch_like'] == False) & (df_slope_all['becomes_positive'] == True) & (df_slope_all['lifetime'] > lifetime_min)), 'category'] = 'gradual_on'\n",
    "    df_slope_all.loc[((df_slope_all['switch_like'] == False) & (df_slope_all['becomes_positive'] == False) & (df_slope_all['born_on'] == False) & (df_slope_all['lifetime'] > lifetime_min)), 'category'] = 'failed_response'\n",
    "    df_slope_all.loc[(df_slope_all['lifetime'] > lifetime_min), 'long_enough_time'] = True\n",
    "    df_slope_all.loc[(df_slope_all['lifetime'] <= lifetime_min), 'long_enough_time'] = False\n",
    "\n",
    "    \n",
    "    return df_slope_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0334b0-15a3-4b0c-8b70-4c0fa8b8cd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slope_df_04 = slope_calculations(ds_all_dict, all_metadata_df, log_gfp_th, slope_threshold = 0.04, duration_thresh = 5, data_var='GFP_median_intensity_processed_transformed', lifetime_min = 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff577c5b-1d13-4d20-9253-9d5e0ade66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code to add the parent start increase time to the slope df\n",
    "\n",
    "lookup = slope_df_04.set_index(['unique_ID', 'cell_id'])['start_inc']\n",
    "\n",
    "missing_keys = []\n",
    "\n",
    "def safe_parent_lookup(row):\n",
    "    key = (row['unique_ID'], row['parent'])  # Assuming 'parent' refers to 'cell_id' of parent\n",
    "    value = lookup.get(key)\n",
    "    if value is None:\n",
    "        missing_keys.append(key)\n",
    "    return value\n",
    "\n",
    "slope_df_04['parent_start_inc'] = slope_df_04.apply(safe_parent_lookup, axis=1)\n",
    "\n",
    "print(f\"Missing {len(set(missing_keys))} parent lookups (e.g., parent not found):\")\n",
    "print(set(missing_keys))\n",
    "slope_df_04.to_csv(plot_output +'/slope_df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ef3f0-e57a-4ae8-b12f-b7d3aacb298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constitutive_slope_calculations(dataset_dict, metadata, data_var=None):\n",
    "    \"\"\" \n",
    "    Final ruby summary stat calculations\n",
    "    Get magnitude, max intensity, and greatest negative slope values and times in a d\n",
    "    Note there's no filtering here - assumes you're filtering/categorizing on the basis of the GFP dataframe \n",
    "    (ie merging the two eventually and dropping any cells not around for long enough etc)\n",
    "    \"\"\"\n",
    "    all_slope_data = []\n",
    "\n",
    "    for unique_id, ds in dataset_dict.items():\n",
    "        print(f\"Running analysis on lineage: {unique_id}\")\n",
    "        ds_metadata = metadata.loc[metadata[\"unique_ID\"] == unique_id].iloc[0]\n",
    "\n",
    "        slope = ds[data_var].diff(dim='time')\n",
    "        cell_records = []\n",
    "        \n",
    "        for cell in ds.cell.values:\n",
    "            cell_data = ds[data_var].sel(cell=cell)\n",
    "            lifetime = np.count_nonzero(~np.isnan(cell_data))\n",
    "            max_var = cell_data.max(dim='time').values\n",
    "            mean_var = cell_data.mean(dim='time').values\n",
    "            max_var_time = cell_data.idxmax(dim='time').values\n",
    "        \n",
    "            slope_cell = slope.sel(cell=cell)\n",
    "            valid_slope = slope_cell.where(~np.isnan(slope_cell), drop=True)\n",
    "        \n",
    "            if valid_slope.size > 0:\n",
    "                min_slope = valid_slope.min(dim='time').values\n",
    "                min_slope_idx = valid_slope.argmin(dim='time').values\n",
    "                time_of_min_slope = ds['time'].isel(time=min_slope_idx + 1).values  # +1 because diff shifts data\n",
    "            else:\n",
    "                min_slope = np.nan\n",
    "                time_of_min_slope = np.nan\n",
    "\n",
    "\n",
    "            cell_records.append({\n",
    "                \"unique_ID\": ds_metadata.unique_ID,\n",
    "                \"experiment\": ds_metadata.experiment,\n",
    "                \"position\": ds_metadata.position,\n",
    "                \"track\": ds_metadata.track,\n",
    "                \"cell_id\": cell,\n",
    "                \"min_slope_ruby\": min_slope,\n",
    "                \"time_of_min_slope_ruby\": time_of_min_slope,\n",
    "                \"lifetime\": lifetime, \n",
    "                \"mean_ruby_lifetime\" : mean_var,\n",
    "                \"max_ruby\": max_var, \n",
    "                \"max_ruby_time\": max_var_time,\n",
    "            })\n",
    "\n",
    "        all_slope_data.append(pd.DataFrame(cell_records))\n",
    "\n",
    "    df_slope_all = pd.concat(all_slope_data, ignore_index=True)\n",
    "    \n",
    "    return df_slope_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb07cd6-d57b-4bd5-bfde-79c95987ccf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ruby_feature_df = constitutive_slope_calculations(ds_all_dict, all_metadata_df, data_var = 'TRITC_median_intensity_processed_transformed')\n",
    "ruby_feature_df.to_csv(plot_output + '/ruby_features_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587ac08-4660-4501-86b3-08981c5a868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_gen(dataset_dict, metadata, length_variable):\n",
    "    \"\"\" \n",
    "    dataset_dict is the dictionary of all the xarray datasets for each position\n",
    "    metadata is the df that stores all the metadata for each positions\n",
    "    \"\"\"\n",
    "    \n",
    "    all_gen_data = []\n",
    "    for unique_id, ds in dataset_dict.items():\n",
    "        \n",
    "        print(f\"Running analysis on lineage: {unique_id}\")\n",
    "        # ds is now our dataset - we don't need to select it\n",
    "        # first find the corresponding metadata - we could split the key name but I feel like this could lead to unanticipated problems\n",
    "        ds_metadata = metadata.loc[metadata[\"unique_ID\"] == unique_id].iloc[0]\n",
    "\n",
    "        # loop through cells and get length data\n",
    "        cell_records = []\n",
    "\n",
    "        for cell_id in ds.cell.values:\n",
    "            length_data = ds[length_variable].sel(cell=cell_id)\n",
    "            #print(\"Length Data\", length_data)\n",
    "            if length_data.notnull().sum().item() == 0:\n",
    "                print(f\"Skipping cell {cell_id} â€” all NaN\")\n",
    "                continue\n",
    "            non_nan_times = length_data['time'].where(length_data.notnull(), drop=True)\n",
    "            first_time = non_nan_times[0].values\n",
    "\n",
    "            \n",
    "            # find daughters for cell and when they appear\n",
    "            daughter_data = ds['daughters'].sel(cell=cell_id)\n",
    "            parent = ds['parents'].sel(cell=cell_id).values.item()\n",
    "            division_indices = np.where((daughter_data.values != 0) & (~np.isnan(daughter_data.values)))[0]\n",
    "            daughter_appearance_times = daughter_data['time'].values[division_indices]\n",
    "            length_data = ds[length_variable].sel(cell=cell_id)\n",
    "            print(\"Cell ID:\", cell_id)\n",
    "            #print(\"Time values:\", daughter_data['time'].values)\n",
    "            print(\"Division indices:\", division_indices)\n",
    "            print(\"Daughter appearance times:\", daughter_appearance_times)\n",
    "            first_time = length_data.notnull().idxmax(dim=\"time\").values.item()\n",
    "            last_time = length_data.notnull()[::-1].idxmax(dim=\"time\").values.item()\n",
    "            print(\"First time:\", first_time)\n",
    "            print(\"Last time:\", last_time)\n",
    "            \n",
    "            # Collect start and end times for each cycle\n",
    "            cycle_times = [first_time] + list(daughter_appearance_times) #values, not indices\n",
    "            print(\"# of Gens\", len(cycle_times))\n",
    "             \n",
    "            for i in range(len(cycle_times)):\n",
    "                start_time = cycle_times[i]  # Value of cycle time, not index\n",
    "                cycle = i\n",
    "                if i < len(cycle_times) - 1:\n",
    "                    end_time = cycle_times[i + 1] - 5# keep coordinate value, note this is 1 time frame (5 min) from the start of the next cycle, change 5 if you have diff intervals\n",
    "                else:\n",
    "                    end_time = last_time  # last coordinate value for the cell\n",
    "                print(\"End Time:\", end_time)\n",
    "                start_index = np.argmin(np.abs(length_data['time'].values - start_time))\n",
    "                end_index = np.argmin(np.abs(length_data['time'].values - end_time))\n",
    "                \n",
    "                start_length = length_data.isel(time=start_index).values\n",
    "                end_length = length_data.isel(time=end_index).values\n",
    "                print(\"Start Index:\", start_index)\n",
    "                print(\"End Index:\", end_index)\n",
    "                \n",
    "                # Access length values \n",
    "                start_length = length_data.isel(time=start_index).values.item()\n",
    "                end_length = length_data.isel(time=end_index).values.item()\n",
    "                print(start_length)\n",
    "                print(end_length)\n",
    "        \n",
    "                cell_records.append({\n",
    "                    \"unique_ID\": ds_metadata.unique_ID,\n",
    "                    \"experiment\": ds_metadata.experiment,\n",
    "                    \"channel_width\": ds_metadata.channel_width,\n",
    "                    \"last_valid_cell_time\": last_time,\n",
    "                    \"first_valid_cell_time\": first_time,\n",
    "                    \"position\": ds_metadata.position,\n",
    "                    \"track\": ds_metadata.track,\n",
    "                    \"cell_id\": cell_id,\n",
    "                    \"parent\":parent,\n",
    "                    \"cycle\": cycle,\n",
    "                    \"start_time\": start_time,\n",
    "                    \"start_length\": start_length,\n",
    "                    \"end_time\": end_time,\n",
    "                    \"end_length\": end_length\n",
    "                })\n",
    "        \n",
    "        all_gen_data.append(pd.DataFrame(cell_records))\n",
    "    \n",
    "    df_all_gen = pd.concat(all_gen_data, ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "    return df_all_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e21dc7-7644-4fb6-b396-9530d8f0aa93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_gen = make_df_gen(ds_all_dict, all_metadata_df, 'feret_diameter_um')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36323706-cc90-442b-958f-54960497a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen['cycle_duration'] = df_gen['end_time']-df_gen['start_time']\n",
    "df_gen['total_growth'] = df_gen['end_length']-df_gen['start_length']\n",
    "df_gen['avg_elong_rate'] = df_gen['total_growth']/df_gen['cycle_duration']\n",
    "df_gen.to_csv(plot_output + '/df_gen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342d7f7-7747-4886-82f9-e1c8e1bf8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [slope_df_005, slope_df_01, slope_df_03, slope_df_04, slope_df_05, slope_df_07, slope_df_09, slope_df_11] \n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "grouped = combined_df.groupby(['slope_th', 'category']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot the table to get categories as columns\n",
    "pivot_df = grouped.pivot(index='slope_th', columns='category', values='count').fillna(0)\n",
    "\n",
    "# Plot as stacked bar\n",
    "pivot_df.plot(kind='bar', stacked=True, figsize=(10, 6), colormap = 'tab20b')\n",
    "\n",
    "plt.xlabel('slope threshold')\n",
    "plt.ylabel('Count')\n",
    "#plt.title('Stacked Bar Plot of Categories by df_id')\n",
    "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_output +'/slope_cat_comparison.pdf',bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4c621-2095-47ae-982c-1308ca4fe7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = ['born_on', 'becomes_positive', 'long_enough_time', 'switch_like',]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Create a new column that captures the boolean combination as a string\n",
    "combined_df['combo'] = combined_df[bool_cols].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Group by df_id and combo\n",
    "grouped = combined_df.groupby(['slope_th', 'combo']).size().reset_index(name='count')\n",
    "\n",
    "# Pivot for stacked bar plot\n",
    "pivot_df = grouped.pivot(index='slope_th', columns='combo', values='count').fillna(0)\n",
    "\n",
    "# Plot\n",
    "pivot_df.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "\n",
    "plt.xlabel('Slope Threshold')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Born On, Becomes Positive, Around Long Enough, Switch Like',loc='best', bbox_to_anchor=(1, 1))\n",
    "#plt.tight_layout()\n",
    "plt.savefig(plot_output +'/slope_comp_allbooleans.pdf',bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a6df5-3aed-48f1-9f65-69a4a743c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting select positions for representative trace plots\n",
    "switch_keys_to_include = { '20240809__XY10_crop7__Track_11', '20240809__XY04_crop3__Track_3', '20240809__XY14_crop2__Track_7' }\n",
    "gradual_keys_to_include = { '20240918__XY12_crop9__Track_25', '20241028__XY06_crop5__Track_39', '20241028__XY17_crop1__Track_0' }\n",
    "non_res_keys_to_include = {'20240918__XY08_crop4__Track_10', '20240918__XY10_crop5__Track_12', '20241028__XY08_crop3__Track_5'}\n",
    "switch_dict = {k: ds_all_dict[k] for k in switch_keys_to_include}\n",
    "gradual_dict = {k: ds_all_dict[k] for k in gradual_keys_to_include}\n",
    "non_res_dict = {k: ds_all_dict[k] for k in non_res_keys_to_include}\n",
    "sample_dicts = [switch_dict, gradual_dict, non_res_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db280b7d-0392-4607-9b4e-38f56f8fefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colormaps as cm\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "\n",
    "# Layout and figure config\n",
    "rows, cols = 1, 3\n",
    "figsize = (15, 3)\n",
    "\n",
    "# Determine global y-limits across all sample_dicts\n",
    "ymin, ymax = np.inf, -np.inf\n",
    "for sample_dict in sample_dicts:\n",
    "    for ds in sample_dict.values():\n",
    "        for cell in ds.cell.values:\n",
    "            parent_value = ds.parents.sel(cell=cell)\n",
    "            if parent_value == 0:\n",
    "                y_vals = ds[\"GFP_median_intensity_processed_transformed\"].sel(cell=cell).values\n",
    "                ymin = min(ymin, np.nanmin(y_vals))\n",
    "                ymax = max(ymax, np.nanmax(y_vals))\n",
    "\n",
    "with PdfPages('/Users/Madison/Desktop/0527_test.pdf') as pdf:\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    plot_idx = 0\n",
    "\n",
    "    for sample_dict in sample_dicts:\n",
    "        ax = axes[plot_idx]\n",
    "\n",
    "        # Assign a unique color per unique_id\n",
    "        unique_ids = sorted(sample_dict.keys())\n",
    "        cmap = plt.colormaps['viridis_r'].resampled(len(unique_ids))  # correct for newer versions\n",
    "        color_map = {uid: cmap(i) for i, uid in enumerate(unique_ids)}\n",
    "\n",
    "        for unique_id in unique_ids:\n",
    "            ds = sample_dict[unique_id]\n",
    "            for cell in ds.cell.values:\n",
    "                parent_value = ds.parents.sel(cell=cell)\n",
    "                if parent_value == 0:\n",
    "                    var_values = ds[\"GFP_median_intensity_processed_transformed\"].sel(cell=cell)\n",
    "                    ax.plot(ds.time, var_values, color=color_map[unique_id], alpha=0.8)\n",
    "\n",
    "        # Axes formatting\n",
    "        ax.set_title(f\"Sample {plot_idx + 1}\", fontsize=8)\n",
    "        ax.set_xlabel(\"Time (mins)\", fontsize=6)\n",
    "        ax.set_ylabel(\"GFP_median_intensity_processed_transformed\", fontsize=6)\n",
    "        ax.tick_params(labelsize=6)\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "        time_values = ds.time.values  # assumes consistent time across samples\n",
    "        if hasattr(time_values[0], 'astype'):  # likely a numpy.datetime64\n",
    "            time_values = time_values.astype(float)\n",
    "        else:\n",
    "            time_values = np.array(time_values)\n",
    "        tick_interval = 120\n",
    "        xticks = np.arange(time_values.min(), time_values.max() + tick_interval, tick_interval)\n",
    "        ax.set_xticks(xticks)\n",
    "        plot_idx += 1\n",
    "\n",
    "        if plot_idx == rows * cols:\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "            axes = axes.flatten()\n",
    "            plot_idx = 0\n",
    "\n",
    "    # Save remaining plots\n",
    "    if plot_idx > 0:\n",
    "        for i in range(plot_idx, rows * cols):\n",
    "            fig.delaxes(axes[i])\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0de01-238a-46a2-af1b-1755216d8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lineages_to_pdf_with_switch_detection(dataset_dict, metadata, firstcross_df, var_to_plot=\"GFP_median_intensity_processed_transformed\",  pdf_filename=\"plots.pdf\",  plots_per_page=16):\n",
    "    \"\"\"\n",
    "    Modification of the plotting code function so that it plots the start, end times, and TH crossing as produced by the slope function as full pdf of plots to check all of them\n",
    "    some of the plotting features are currently hardcoded - rows, cols, figsize - so you may want to change these\n",
    "    \"\"\"\n",
    "    rows, cols = 4, 4\n",
    "    with PdfPages(pdf_filename) as pdf:\n",
    "       \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(16, 16))\n",
    "        axes = axes.flatten()\n",
    "        plot_idx = 0\n",
    "        \n",
    "        for unique_id, ds in dataset_dict.items():\n",
    "            y_val_dict={}\n",
    "            ax = axes[plot_idx]\n",
    "            print(unique_id)\n",
    "            ds_metadata = metadata.loc[metadata[\"unique_ID\"] == unique_id].iloc[0]\n",
    "            for cell in ds.cell.values:\n",
    "                var_values = ds[var_to_plot].sel(cell=cell)\n",
    "                parent_value = ds.parents.sel(cell=cell)\n",
    "                if parent_value ==0:\n",
    "                    color = 'limegreen'\n",
    "                else: \n",
    "                    color = 'lightgrey' \n",
    "                alpha = 1 if ds.parents.sel(cell=cell)==0 else 0.5\n",
    "                linewidth = 2 if ds.parents.sel(cell=cell)==0 else 0.75\n",
    "                ax.plot(ds.time, var_values, color=color, alpha=alpha, linewidth = linewidth)\n",
    "\n",
    "            # Highlight switch points (start and end)\n",
    "                switch_info = firstcross_df[(firstcross_df['unique_ID'] == unique_id) & (firstcross_df['cell_id'] == cell)]\n",
    "                \n",
    "                if not switch_info.empty:\n",
    "                    if cell ==1:\n",
    "                        row = switch_info.iloc[0]\n",
    "                        for switch_time, color, marker, label in [\n",
    "                        ('start_inc', 'forestgreen', 'o', 'Start'),\n",
    "                        ('gfp_on', 'black', '.', 'TH'),\n",
    "                        ('end_inc', 'magenta', 'o', 'End'),\n",
    "                        ]:\n",
    "                            if pd.notna(row[switch_time]):\n",
    "            # Find index in time array closest to the switch time\n",
    "                                switch_t = row[switch_time]\n",
    "                                if switch_t in ds.time.values:\n",
    "                                    y_val = var_values.sel(time=switch_t)\n",
    "                                    print(f\"{label} ({switch_time}) at {switch_t}: y_val = {float(y_val.values)}\")\n",
    "                                    ax.scatter(switch_t, y_val.values, color=color, marker=marker, s=50, linewidth = 2, label=label) \n",
    "\n",
    "                if cell !=1:\n",
    "                            row = switch_info.iloc[0]\n",
    "                            for switch_time, color, marker, label in [\n",
    "                            ('start_inc', 'darkgreen', 'X', 'Start'),\n",
    "                            ('gfp_on', 'gray', '.', 'TH'),\n",
    "                            ('end_inc', 'purple', 'X', 'End'),\n",
    "                            ]:\n",
    "                                if pd.notna(row[switch_time]):\n",
    "                 #Find index in time array closest to the switch time\n",
    "                                    switch_t = row[switch_time]\n",
    "                                    if switch_t in ds.time.values:\n",
    "                                        y_val = var_values.sel(time=switch_t)\n",
    "                                        print(f\"{label} ({switch_time}) at {switch_t}: y_val = {float(y_val.values)}\")\n",
    "                                        ax.scatter(switch_t, y_val.values, color=color, marker=marker, alpha=0.5, linewidth = 0.75, s=50, label=label)\n",
    "\n",
    "            ax.set_title(f\"{unique_id}\", fontsize=8)\n",
    "            ax.set_xlabel(\"Time (mins)\", fontsize=6)\n",
    "            #ax.set_yscale('log')\n",
    "            ax.set_ylabel(var_to_plot, fontsize=6)\n",
    "            ax.tick_params(labelsize=6)\n",
    "            plot_idx += 1\n",
    "\n",
    "            # If page is full, save and reset\n",
    "            if plot_idx == plots_per_page:\n",
    "                plt.tight_layout()\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "                fig, axes = plt.subplots(rows, cols, figsize=(16, 16))\n",
    "                axes = axes.flatten()\n",
    "                plot_idx = 0\n",
    "\n",
    "        # Save remaining plots\n",
    "        if plot_idx > 0:\n",
    "            for i in range(plot_idx, plots_per_page):\n",
    "                fig.delaxes(axes[i])\n",
    "            plt.tight_layout()\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"Saved plots to {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6d745-f246-4c86-a2a7-f2191d601b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lineages_to_pdf_with_switch_detection(ds_all_dict, all_metadata_df, slope_df_04, var_to_plot=\"GFP_median_intensity_processed_transformed\",  pdf_filename=plot_output + \"/final_lineages_with_switch.pdf\",  plots_per_page=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmm",
   "language": "python",
   "name": "delta_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
